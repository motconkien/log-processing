# GitHub Logs Processing Pipeline
![Pipeline Architecture](https://drive.google.com/file/d/1AgWepLxou1keuVm4AVkmIVg27lfereIu/view?usp=share_link)


This project is an end-to-end data pipeline that retrieves public event logs from the GitHub API, stores them in local files, uploads to AWS S3 and transfer to Redshift, can be quired by Athena or Redshift

## ğŸ“¦ Project Structure
```
GitHub-logs-processing/
â”œâ”€â”€ configs/
â”‚ â””â”€â”€ config.json # Configuration file (API tokens, AWS keys, paths)
â”œâ”€â”€ dags/
â”‚ â””â”€â”€ logs_dag.py # Apache Airflow DAG to orchestrate the pipeline
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ logs/ # Stored GitHub logs data
â”‚ â””â”€â”€ users/ # Stored GitHub users data
â”œâ”€â”€ pipelines/
â”‚ â”œâ”€â”€ log_pipeline.py # Fetch and process GitHub logs
â”‚ â”œâ”€â”€ users_pipeline.py # Fetch and process GitHub user data
â”‚ â””â”€â”€ aws_pipeline.py # Upload data to AWS S3 
â”œâ”€â”€ utils/
â”‚ â””â”€â”€ constants.py # Constants used across the project
â”œâ”€â”€ airflow.env # Environment variables for Airflow
â”œâ”€â”€ docker-compose.yml # Docker compose file for services
â”œâ”€â”€ Dockerfile # Dockerfile for building the project image
â”œâ”€â”€ requirements.txt # Python dependencies list
â””â”€â”€ README.md # Project documentation
```


## ğŸš€ Features

- ğŸ” Automatically fetch GitHub event logs using the GitHub API daily
- ğŸ§¹ Clean and structure logs for storage
- ğŸ›¢ Upload logs to:
  - Local storage (for audit/debugging)
  - Amazon S3
- â›“ï¸ Orchestrated by Apache Airflow
- ğŸ§ª Ready for extension to Amazon Redshift

## ğŸ›  Setup Instructions

```bash
git clone https://github.com/your-username/github-logs-processing.git
cd github-logs-processing
docker-compose build
docker-compose run --rm airflow-init
docker-compose up -d
```